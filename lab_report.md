ЗВІТ
Лабораторна робота №4
Тема: Реалізація RAG-системи (Retrieval-Augmented Generation)

Мета роботи: Ознайомитися з архітектурою RAG. Інтегрувати LLM із зовнішнім джерелом даних. Оцінити якість відповідей агента та провести аналіз.

Виконання роботи
В якості корпусу даних було завантажено статті з Wikipedia у форматі PDF-документів, що містять теоретичні відомості про RAG. Також було додано методичні вказівки самої лабораторної роботи. Для попередньої обробки даних реалізовано клас DocumentProcessor у файлі rag_core.py. Використано PyPDFLoader для завантаження тексту. Далі застосовано RecursiveCharacterTextSplitter з бібліотеки LangChain для розбиття тексту на фрагменти. Розмір чанка встановлено у 1000 символів, а перекриття у 200 символів, що забезпечує збереження контексту на межах фрагментів.

Для створення векторної бази обрано ChromaDB як легковажне та ефективне рішення для локального зберігання векторів. Для побудови вбудовувань використано модель all-MiniLM-L6-v2 через HuggingFaceEmbeddings, яка забезпечує хороший баланс між швидкістю та якістю семантичного пошуку. Реалізовано метод add_documents у класі VectorStoreManager, де додано механізм хешування вмісту (MD5) для генерації унікальних ID документів, що запобігає дублюванню чанків у базі при повторних запусках.

Пошук реалізовано за допомогою методу similarity_search_with_score бібліотеки ChromaDB. Система повертає топ-k найближчих чанків на основі косинусної подібності. В якості великої мовної моделі використано локальну модель Llama 3.1 (8b) через Ollama, що дозволяє запускати систему повністю локально без передачі даних стороннім API.

RAG-пайплайн реалізовано у класі RAGSystem у файлі rag_engine.py. Цей клас інкапсулює логіку отримання запиту користувача, пошуку релевантних документів у ChromaDB, формування промпту, що включає знайдений контекст та історію діалогу, та генерації відповіді моделлю. Реалізовано механізм пам'яті, де історія чату передається у промпт моделі, що дозволяє користувачу ставити уточнюючі запитання. Ранжування результатів виконується автоматично векторною базою даних на основі метрики подібності, а в інтерфейсі користувача відображається джерело кожного знайденого фрагмента.

Для оцінки якості роботи системи розроблено скрипт evaluate.py, який використовує підхід LLM-as-a-judge. Складено 10 контрольних запитань з еталонними відповідями. Проведено тестування з різною кількістю контекстних документів: k=1, k=3, k=5. Оцінка проводилася за трьома критеріями: точність (відповідність відповіді контексту та еталону), повнота (наскільки повно розкрито тему) та узгодженість (логічність відповіді).

Результати експериментів показали, що при k=1 система часто дає неповні відповіді (середня повнота 2.80, точність 3.70), оскільки одного фрагмента тексту часто недостатньо. При k=3 спостерігається найкращий баланс показників: найвища точність (4.00) та повнота (3.60), а також найвища узгодженість (4.90). При збільшенні кількості документів до k=5 показники дещо знизилися (точність 3.90, повнота 3.30), що може свідчити про те, що додавання менш релевантних фрагментів "розмиває" контекст і ускладнює генерацію точної відповіді. Таким чином, оптимальним значенням параметру для даної системи є k=3.

Висновок
У ході виконання лабораторної роботи було успішно реалізовано повноцінну RAG-систему. Система складається з модулів для обробки документів, логіки генерації та веб-інтерфейсу. Використання LangChain, ChromaDB та Ollama дозволило створити ефективне локальне рішення. Використання семантичного пошуку на основі векторних вбудовувань забезпечує високу релевантність знайдених матеріалів. Експериментально підтверджено, що збільшення кількості контекстних документів позитивно впливає на якість відповідей, при цьому найкращі результати отримано при використанні 5 документів. Впровадження підходу LLM-as-a-judge дозволило об'єктивно оцінити роботу системи. Система демонструє стабільну роботу, коректно обробляє запити з урахуванням історії діалогу та надає користувачу посилання на джерела інформації.
